{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ackmase/sandbox/blob/main/Fraudulent_GenAI_Receipt_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGD7tCMV_0cb"
      },
      "outputs": [],
      "source": [
        "# Get necessary packages.\n",
        "# https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html\n",
        "# Note that Google CoLab has the following system set up:\n",
        "# - python version: 3.12.11\n",
        "# - pip version: 24.1.2\n",
        "# - platform: 64 bit, x86_64\n",
        "# - CPU: Intel Xeon CPU @ 2.00GHz\n",
        "# - Nvidia GPU: T4\n",
        "# - cuda version: 12.5.82\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install paddlepaddle-gpu==3.1.1 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n",
        "!pip install paddleocr\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers accelerate==1.3.0 openai pytesseract easyocr keras_ocr datasets\n",
        "!pip install diffusers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oDrfTxvRdAJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fksuFH9JMrjG"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "# Third-party libraries\n",
        "import gradio as gr\n",
        "from diffusers import DiffusionPipeline\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from openai import OpenAI\n",
        "from paddleocr import PaddleOCR\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from transformers import pipeline, VisionEncoderDecoderModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YmWmvpDEMv9v"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "openai_token = userdata.get('openAI')\n",
        "openai = OpenAI(api_key=openai_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TOSTaMhyM1RV"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "from datetime import date\n",
        "\n",
        "today_str = date.today().isoformat()\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a helpful receipt checker for a company that gives out rewards based \\\n",
        "on receipts submitted by customers. You look at OCR text extracted from these \\\n",
        "receipts and determine whether you think it looks real or not. Remember that \\\n",
        "OCR extraction is still imperfect, and even real receipts can have \\\n",
        "imperfections. Also, watch out! Fraudsters are getting better at faking \\\n",
        "receipts these days.\n",
        "\n",
        "Today's date is {today_str}.\n",
        "\n",
        "Given OCR receipt text, please answer with one of the following options:\n",
        "Real\n",
        "Fake\n",
        "Needs Human Validation\n",
        "\n",
        "Then explain why you think so.\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT = \"\"\"\n",
        "You are given imperfect OCR text extractions from photos snapped by a mobile \\\n",
        "phone. When determining the veracity of the receipt, please ignore the \\\n",
        "following:\n",
        "- all formatting and spacing\n",
        "- weird OCR artifacts (e.g.,  or odd character strings like 'arr   el)')\n",
        "- odd characters\n",
        "\n",
        "Your job is to try and figure out if the receipt text is real or fradulent. \\\n",
        "Watch out! Fraudsters are getting better at faking receipts these days.\n",
        "\n",
        "Here's an example of a real receipt:\n",
        "CVS pharmacy2115 ARIESIA BLVD SUITE 100\n",
        "REDONDO BEACH,CA 90278\n",
        "310.214.3974\n",
        "REG#18 TRN#6711 CSHR#0000098 STR#10795\n",
        "ExtraCare Card H: ********3Y00\n",
        "POST GRP NUTS CRL Z0.5 3.99F\n",
        "ORIGINAL PRICE 6.49\n",
        "3.49 EACH 3.00\n",
        "Survey_ID #\n",
        "6469 3692 6776 093 75\n",
        "TOTAL 3.49\n",
        "CHARGE 3.49\n",
        "************2496 RF\n",
        "VISA CREDIT *************2496\n",
        "RPPHOVEDE 757900 REF# 187119\n",
        "TRAN TYPE: SALE AID: A0000000031010\n",
        "TC:_1999600P97E1FZ51 TERMINPL#\n",
        "NO SIGNATURE REQUIRED CVM: 280000\n",
        "TSI(98): 0000\n",
        "TVRC95): 0000000000\n",
        ".00\n",
        "CHANGE\n",
        "3510 7955 2276 7111 88\n",
        "Returns with receipt, subJect to\n",
        "cVS Return Po11cy, thru 10/14/2025\n",
        "Refund anount is based on Price\n",
        "after all coupons and discounts.\n",
        "9:37 AM\n",
        "AUGUST 15, 2025\n",
        "L\n",
        "14 no-cost vaccines\n",
        "avallable with most insurance\n",
        "Protect yourself against RSV,\n",
        "shingles, Tdap & more.\n",
        "Subject to availability.\n",
        "Restrictions apply.\n",
        "Scan the QR to schedule your vaccination.\n",
        "\n",
        "Note that the real receipt has some jargon and abbreviations and spelling \\\n",
        "errors.\n",
        "\n",
        "Here's an example of a fake receipt:\n",
        "SEASIDE MARKET\n",
        "8/6/2025\n",
        "TRIX 3.29\n",
        "OATMEAL CRISP 4.79\n",
        "TOTAL 8.08\n",
        "\n",
        "Note that the fake receipt is lacking important information typically found in \\\n",
        "a transaction like cash tendered or masked credit card information.\n",
        "\n",
        "Please answer with one of the following options:\n",
        "Real\n",
        "Fake\n",
        "Needs Human Validation\n",
        "\n",
        "Then explain why you think so.\n",
        "\n",
        "Here is OCR text extracted from a receipt I'd like you to review:\\n\\n\")\n",
        "\"\"\"\n",
        "\n",
        "GEMMA_MODEL = \"google/gemma-3-270m\"\n",
        "LIQUID_MODEL = \"LiquidAI/LFM2-1.2B\"\n",
        "NVIDIA_MODEL = \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\"\n",
        "LFM2_MODEL = \"LiquidAI/LFM2-VL-450M\"\n",
        "DOCLING_MODEL = \"ds4sd/SmolDocling-256M-preview\"\n",
        "TR_OCR_MODEL = \"microsoft/trocr-base-printed\"\n",
        "GPT_MODEL = \"gpt-4o-mini\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4R1zB2gM37_"
      },
      "outputs": [],
      "source": [
        "# Function for checking the veracity of the receipt image\n",
        "\n",
        "def DetermineVeracityOfReceiptText(text):\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_PROMPT + text}\n",
        "        ]\n",
        "    stream = openai.chat.completions.create(\n",
        "                model=GPT_MODEL,\n",
        "                messages=messages,\n",
        "                stream=True\n",
        "                )\n",
        "    result = \"\"\n",
        "    for chunk in stream:\n",
        "        result += chunk.choices[0].delta.content or \"\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR_Ba0HiMoVE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Helper functions for formatting PaddleOCR output into readable text.\n",
        "\n",
        "Usage:\n",
        "\n",
        "paddleocr_ppocrv5_to_text(\n",
        "    res, # JSON result from PaddleOCR\n",
        "    y_threshold=5) # Vertical proximity threshold to group words into lines\n",
        "\n",
        "Note that for receipts, where each line is very close to adjacent lines, it\n",
        "seems that y_threshold=5 is the right balance to get the correct formatting.\n",
        "Increase this number to 10 or 15 if too many words are getting grouped together\n",
        "into single lines.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "\n",
        "def paddleocr_ppocrv5_to_text(ocr_result, y_threshold=15):\n",
        "    \"\"\"\n",
        "    Convert PaddleOCR PP-OCRv5 JSON output to readable string with line breaks and spaces.\n",
        "\n",
        "    Args:\n",
        "        ocr_result (dict): Parsed JSON result from PaddleOCR.\n",
        "        y_threshold (int): Vertical proximity threshold to group words into lines.\n",
        "\n",
        "    Returns:\n",
        "        str: Reconstructed OCR text.\n",
        "    \"\"\"\n",
        "    boxes = ocr_result[\"rec_boxes\"]\n",
        "    texts = ocr_result[\"rec_texts\"]\n",
        "\n",
        "    # Build a list of (box, text) tuples with (x_min, y_min, x_max, y_max)\n",
        "    word_entries = []\n",
        "    for box, text in zip(boxes, texts):\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "        word_entries.append({\n",
        "            \"text\": text,\n",
        "            \"x\": x_min,\n",
        "            \"y\": y_min,\n",
        "            \"line_center\": (y_min + y_max) / 2\n",
        "        })\n",
        "\n",
        "    # Group words into lines based on vertical proximity\n",
        "    lines = []\n",
        "    for word in sorted(word_entries, key=lambda x: x[\"line_center\"]):\n",
        "        placed = False\n",
        "        for line in lines:\n",
        "            if abs(line[\"avg_y\"] - word[\"line_center\"]) <= y_threshold:\n",
        "                line[\"words\"].append(word)\n",
        "                line[\"avg_y\"] = sum(w[\"line_center\"] for w in line[\"words\"]) / len(line[\"words\"])\n",
        "                placed = True\n",
        "                break\n",
        "        if not placed:\n",
        "            lines.append({\"avg_y\": word[\"line_center\"], \"words\": [word]})\n",
        "\n",
        "    # Sort words in each line by x position\n",
        "    output_lines = []\n",
        "    for line in lines:\n",
        "        sorted_words = sorted(line[\"words\"], key=lambda w: w[\"x\"])\n",
        "        output_lines.append(\" \".join(w[\"text\"] for w in sorted_words))\n",
        "\n",
        "    return \"\\n\".join(output_lines)"
      ],
      "metadata": {
        "id": "DPs2tLxXHwtF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for using PaddleOCR\n",
        "def PullOCRText(image):\n",
        "    # Resize image to make inference cheaper\n",
        "    w, h = image.size\n",
        "    resized_image = image.resize((w // 4, h // 4), Image.LANCZOS)\n",
        "\n",
        "    # Convert image into numpy.ndarray\n",
        "    print(\"logging: resizing image\")\n",
        "    resized_image = np.array(resized_image)\n",
        "    ocr = PaddleOCR(\n",
        "        lang=\"en\",\n",
        "        use_doc_orientation_classify=False,\n",
        "        use_doc_unwarping=False,\n",
        "        use_textline_orientation=False,\n",
        "        )\n",
        "\n",
        "    # Run OCR inference on a sample image\n",
        "    print(\"logging: running OCR inference\")\n",
        "    result = ocr.predict(resized_image)\n",
        "    print(\"logging: OCR inference complete\")\n",
        "\n",
        "    # Construct output and return\n",
        "    #drive.mount('/content/drive')\n",
        "    output = \"\"\n",
        "    for res in result:\n",
        "        res.print()\n",
        "        print(\"logging: constructing output\")\n",
        "        output += paddleocr_ppocrv5_to_text(res, y_threshold=5) #10) #15)\n",
        "        print(\"logging: returning output\")\n",
        "    return output"
      ],
      "metadata": {
        "id": "D3hGsUvOoKg9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kU5TVFXheWw"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# Note: plug in your real implementation\n",
        "# ---------------------------------------------\n",
        "\n",
        "def _fallback_PullOCRText(image):\n",
        "    \"\"\"\n",
        "    Fallback OCR (placeholder). Replace with your real PullOCRText(image)->str.\n",
        "    \"\"\"\n",
        "    return (\"[Fallback OCR] (Replace with PullOCRText) — No OCR engine wired. \"\n",
        "           \"If you already have PullOCRText(image)->str, import it and remove \"\n",
        "           \"this fallback.\")\n",
        "\n",
        "def _fallback_DetermineVeracityOfReceiptText(text):\n",
        "    \"\"\"\n",
        "    Fallback veracity explanation (placeholder). Replace with your real\n",
        "    DetermineVeracityOfReceiptText(text)->str. Should return an explanation\n",
        "    string; Cell 4 will parse a final label from this text.\n",
        "    \"\"\"\n",
        "    # Extremely naive example logic — replace with your model/rules:\n",
        "    return (\"[Fallback veracity explanation] Replace with your real \"\n",
        "            \"DetermineVeracityOfReceiptText\")\n",
        "\n",
        "# Try to use user-provided functions if present; otherwise use fallbacks\n",
        "try:\n",
        "    PullOCRText  # type: ignore\n",
        "except NameError:\n",
        "    PullOCRText = _fallback_PullOCRText\n",
        "\n",
        "try:\n",
        "    DetermineVeracityOfReceiptText  # type: ignore\n",
        "except NameError:\n",
        "    DetermineVeracityOfReceiptText = _fallback_DetermineVeracityOfReceiptText\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Helpers\n",
        "# ---------------------------------------------\n",
        "FINAL_LABELS = [\"Real\", \"Fake\", \"Needs Human Validation\"]\n",
        "\n",
        "def extract_final_label(explanation_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the final determination from the explanation text.\n",
        "    Looks for one of: [\"Real\", \"Fake\", \"Needs Human Validation\"].\n",
        "    If none found, defaults to \"Needs Human Validation\".\n",
        "    \"\"\"\n",
        "    if not explanation_text:\n",
        "        return \"Needs Human Validation\"\n",
        "\n",
        "    # Normalize whitespace for robust matching\n",
        "    text_norm = \" \".join(explanation_text.split()).lower().split()\n",
        "    # Look for explicit tokens (case-insensitive)\n",
        "    if \"real\" in text_norm[0] or \" real\" in text_norm[0]:\n",
        "        return \"Real\"\n",
        "    if \"fake\" in text_norm[0] or \" fake\" in text_norm[0]:\n",
        "        return \"Fake\"\n",
        "    text_norm = \" \".join(text_norm)\n",
        "    if (\"needs human validation\" in text_norm or\n",
        "        \"needs review\" in text_norm or\n",
        "        \"needs human validation\" in text_norm):\n",
        "        return \"Needs Human Validation\"\n",
        "\n",
        "    # If tokens not present, attempt a heuristic fallback\n",
        "    for lbl in FINAL_LABELS:\n",
        "        if lbl.lower() in text_norm:\n",
        "            return lbl\n",
        "\n",
        "    return \"Needs Human Validation\"\n",
        "\n",
        "\n",
        "def pipeline(image):\n",
        "    \"\"\"\n",
        "    Main pipeline: (Cell 1) image -> (Cell 2) decision\n",
        "                   -> (Cell 3) OCR text\n",
        "                   -> (Cell 4) full explanation\n",
        "\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        ocr_text = \"\"\n",
        "        explanation = (\"No image provided. Please upload a receipt image. \"\n",
        "                       \"Final: Needs Human Validation\")\n",
        "        decision = \"Needs Human Validation\"\n",
        "        return ocr_text, explanation, decision\n",
        "\n",
        "    # Cell 2: OCR\n",
        "    try:\n",
        "        ocr_text = PullOCRText(image)\n",
        "    except Exception as e:\n",
        "        ocr_text = \"\"\n",
        "        explanation = f\"OCR error: {e}. Final: Needs Human Validation\"\n",
        "        decision = \"Needs Human Validation\"\n",
        "        return ocr_text, explanation, decision\n",
        "\n",
        "    # Cell 3: Explanation via DetermineVeracityOfReceiptText\n",
        "    try:\n",
        "        explanation = DetermineVeracityOfReceiptText(ocr_text)\n",
        "        if not isinstance(explanation, str):\n",
        "            explanation = (f\"[Warning] DetermineVeracityOfReceiptText returned \"\n",
        "                           f\"non-string type ({type(explanation)}). \"\n",
        "                           f\"Final: Needs Human Validation\")\n",
        "    except Exception as e:\n",
        "        explanation = (f\"Error during veracity determination: {e}. \"\n",
        "                       f\"Final: Needs Human Validation\")\n",
        "\n",
        "    # Cell 4: Final label parsed from explanation text\n",
        "    decision = extract_final_label(explanation)\n",
        "    if decision not in FINAL_LABELS:\n",
        "        decision = \"Needs Human Validation\"\n",
        "\n",
        "    return ocr_text, explanation, decision\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# UI\n",
        "# ---------------------------------------------\n",
        "with gr.Blocks(title=\"Receipt Veracity Checker\") as demo:\n",
        "    gr.Markdown(\"## Receipt Veracity Checker\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Left column: Cell 1\n",
        "        with gr.Column(scale=1):\n",
        "            image_input = gr.Image(\n",
        "                label=\"Upload Receipt Image here\",\n",
        "                type=\"pil\"\n",
        "            )\n",
        "            run_button = gr.Button(\"Run Analysis\", variant=\"primary\")\n",
        "\n",
        "        # Right column: Cells 2, 3, 4 stacked\n",
        "        with gr.Column(scale=1):\n",
        "            decision = gr.Radio(\n",
        "                label=\"Decision\",\n",
        "                choices=FINAL_LABELS,\n",
        "                value=\"Needs Human Validation\",\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            ocr_output = gr.Textbox(\n",
        "                label=\"Extracted OCR Text\",\n",
        "                lines=12,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            explanation_output = gr.Textbox(\n",
        "                label=\"Full Explanation\",\n",
        "                lines=12,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    # Wire interactions: click and/or auto-run on image change\n",
        "    run_button.click(\n",
        "        fn=pipeline,\n",
        "        inputs=image_input,\n",
        "        outputs=[ocr_output, explanation_output, decision]\n",
        "    )\n",
        "\n",
        "    image_input.change(\n",
        "        fn=pipeline,\n",
        "        inputs=image_input,\n",
        "        outputs=[ocr_output, explanation_output, decision]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNmkGrxiIkrcfM3K94bZ3O/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}